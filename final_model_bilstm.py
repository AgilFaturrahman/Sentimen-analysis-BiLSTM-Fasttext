# -*- coding: utf-8 -*-
"""Salinan dari model_bilstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12oLxxX3XQ-RxZbp7l5YKUQuCflmIlkCP
"""

!pip install Sastrawi
!pip install nlpaug

# --- 1. Import dan Setup ---
import pandas as pd
import numpy as np
import re
import random
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# --- 2. Konfigurasi Global & Hyperparameter ---
DATA_PATH = '/content/drive/MyDrive/skripsi/cleaned_tweets2.csv'
AUGMENTED_DATA_PATH = '/content/drive/MyDrive/skripsi/eda_augmented_tweets.csv'
ASSET_DIR_BASE = '/content/drive/MyDrive/skripsi/bilstm_scenarios'

# Hyperparameter yang konsisten (Konstanta)
EMBEDDING_DIM = 100
MAX_SEQUENCE_LENGTH = 100
EPOCHS = 30
TEST_SIZE = 0.2
DROPOUT_RATE = 0.2
LSTM_UNITS = 64 # Jumlah unit BiLSTM (dibuat konsisten untuk semua skenario)
NUM_AUGMENTATIONS_PER_SAMPLE = 1

# Hyperparameter yang divariasikan (Skenario)
SCENARIOS = [
    # Skenario 1 (BS 64, LR 0.001) - Baseline Gold Standard
    {'batch_size': 64, 'learning_rate': 0.001, 'name': 'Skenario_1_BS64_LR001'},
    # Skenario 2 (BS 64, LR 0.01) - LR Cepat
    {'batch_size': 64, 'learning_rate': 0.01, 'name': 'Skenario_2_BS64_LR01'},
    # Skenario 3 (BS 64, LR 0.0001) - LR Lambat
    {'batch_size': 64, 'learning_rate': 0.0001, 'name': 'Skenario_3_BS64_LR0001'},
    # Skenario 4 (BS 128, LR 0.001) - Efisien (Mid-Speed)
    {'batch_size': 128, 'learning_rate': 0.001, 'name': 'Skenario_4_BS128_LR001'},
    # Skenario 5 (BS 128, LR 0.01) - Paling Cepat
    {'batch_size': 128, 'learning_rate': 0.01, 'name': 'Skenario_5_BS128_LR01'},
    # Skenario 6 (BS 128, LR 0.0001) - Paling Lambat
    {'batch_size': 128, 'learning_rate': 0.0001, 'name': 'Skenario_6_BS128_LR0001'},
]

# --- 3. Preprocessing (Functions) ---
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Kamus slang
try:
    slang_df = pd.read_csv('/content/drive/MyDrive/skripsi/slang.csv')
    slang_dict = dict(zip(slang_df['slang'], slang_df['formal']))
except Exception:
    slang_dict = {
        'yg':'yang','kmrn':'kemarin','udh':'sudah','tdk':'tidak','jg':'juga','ga':'tidak',
        'gk':'tidak','bgt':'banget','bgtu':'begitu','trs':'terus','jd':'jadi','dr':'dari',
        'klo':'kalau','sm':'sama','gmn':'bagaimana','utk':'untuk','krn':'karena','brp':'berapa',
        'sy':'saya','dn':'dan','skrg':'sekarang','bikin':'membuat','bln':'bulan','pke':'pakai',
        'org':'orang','dgn':'dengan','pdhl':'padahal','ato':'atau','sbnrnya':'sebenarnya',
        'tp':'tapi','jdwal':'jadwal','jgk':'juga','mnrt':'menurut','blm':'belum','bbrp':'beberapa',
        'gt':'begitu','k':'ke','lg':'lagi','tggl':'tinggal','bkn':'bukan'
    }

def clean_and_normalize_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    words = text.split()
    normalized_words = [slang_dict.get(word, word) for word in words]
    text = ' '.join(normalized_words)
    text = stemmer.stem(text)
    return text

# Fungsi EDA Sederhana (tetap sama)
def random_deletion(words, p=0.1):
    if len(words) == 1: return words
    new_words = [w for w in words if random.uniform(0,1) > p]
    return new_words if new_words else [random.choice(words)]

def random_swap(words, n=1):
    new_words = words.copy()
    for _ in range(n):
        if len(new_words) < 2: return new_words
        idx1, idx2 = random.sample(range(len(new_words)), 2)
        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]
    return new_words

def synonym_replacement(words, n=1):
    # Diimplementasikan sebagai penambahan kata acak dari kata yang sudah ada (simplifikasi)
    new_words = words.copy()
    for _ in range(n):
        idx = random.randint(0, len(new_words)-1)
        new_words.insert(idx, new_words[idx])
    return new_words

def eda_augment(text):
    words = text.split()
    if len(words) < 2: return text
    choice = random.choice(['deletion', 'swap', 'synonym'])
    if choice == 'deletion':
        aug_words = random_deletion(words)
    elif choice == 'swap':
        aug_words = random_swap(words)
    else:
        aug_words = synonym_replacement(words)
    return ' '.join(aug_words)

# --- 4. Persiapan Data (Load, Augmentasi, Encode) ---
def prepare_data():
    print("Mempersiapkan data dan tokenizing...")
    df = None

    # Coba muat data augmented
    if os.path.exists(AUGMENTED_DATA_PATH):
        try:
            df = pd.read_csv(AUGMENTED_DATA_PATH)
            if 'cleaned_text' not in df.columns or 'label' not in df.columns:
                raise KeyError("Kolom tidak ditemukan.")
            print(f"Data augmented berhasil dimuat dari {AUGMENTED_DATA_PATH}.")
        except Exception as e:
            print(f"Error saat memuat data augmented: {e}")
            df = None

    # Jika gagal/belum ada, lakukan cleaning dan augmentasi
    if df is None:
        df = pd.read_csv(DATA_PATH)
        df = df[['Text', 'label']]
        df.columns = ['text', 'label']
        df['cleaned_text'] = df['text'].apply(clean_and_normalize_text)
        augmented_data = []
        for _, row in df.iterrows():
            original_text = row['cleaned_text']
            original_label = row['label']
            augmented_data.append({'cleaned_text': original_text, 'label': original_label})
            for _ in range(NUM_AUGMENTATIONS_PER_SAMPLE):
                aug_text = eda_augment(original_text)
                augmented_data.append({'cleaned_text': aug_text, 'label': original_label})
        df = pd.DataFrame(augmented_data)
        df.to_csv(AUGMENTED_DATA_PATH, index=False)
        print(f"Data asli dibersihkan dan augmented, disimpan di: {AUGMENTED_DATA_PATH}")

    # Encoding dan Tokenizer
    label_encoder = LabelEncoder()
    df['encoded_label'] = label_encoder.fit_transform(df['label'])
    num_classes = len(label_encoder.classes_)

    tokenizer = Tokenizer(oov_token="<unk>")
    tokenizer.fit_on_texts(df['cleaned_text'])
    word_index = tokenizer.word_index
    sequences = tokenizer.texts_to_sequences(df['cleaned_text'])
    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')

    # Split Data
    X_train, X_test, y_train, y_test = train_test_split(
        padded_sequences, df['encoded_label'], test_size=TEST_SIZE,
        random_state=42, stratify=df['encoded_label']
    )

    # Simpan test_df untuk keperluan evaluasi eksternal
    test_df = df.loc[y_test.index, ['cleaned_text', 'label']].reset_index(drop=True)
    test_df.to_csv(os.path.join(ASSET_DIR_BASE, 'global_test_data.csv'), index=False)

    print(f"Total data train: {len(X_train)} | Total data test: {len(X_test)}")
    return X_train, X_test, y_train, y_test, tokenizer, label_encoder, num_classes

# --- 5. Definisi Model dan Fungsi Pelatihan ---
def build_model(vocab_size, embedding_dim, max_seq_len, lstm_units, dropout_rate, num_classes):
    input_layer = Input(shape=(max_seq_len,))
    # Menggunakan Keras Embedding layer yang dilatih dari awal (trainable=True)
    embedding_layer = Embedding(
        input_dim=vocab_size,
        output_dim=embedding_dim,
        input_length=max_seq_len,
        trainable=True
    )(input_layer)

    bilstm_layer_1 = Bidirectional(LSTM(lstm_units, return_sequences=False))(embedding_layer)
    dropout_layer = Dropout(dropout_rate)(bilstm_layer_1)
    output_layer = Dense(num_classes, activation='softmax')(dropout_layer)

    model = Model(inputs=input_layer, outputs=output_layer)
    return model

def train_and_evaluate_model(X_train, X_test, y_train, y_test, tokenizer, label_encoder, num_classes, params):
    scenario_name = params['name']
    lr = params['learning_rate']
    bs = params['batch_size']

    print(f"\n--- Memulai {scenario_name} (BS: {bs}, LR: {lr}) ---")
    os.makedirs(os.path.join(ASSET_DIR_BASE, scenario_name), exist_ok=True)

    # Build Model
    model = build_model(
        vocab_size=len(tokenizer.word_index) + 1,
        embedding_dim=EMBEDDING_DIM,
        max_seq_len=MAX_SEQUENCE_LENGTH,
        lstm_units=LSTM_UNITS,
        dropout_rate=DROPOUT_RATE,
        num_classes=num_classes
    )

    # Menggunakan Adam Optimizer dengan Learning Rate yang ditentukan
    optimizer = Adam(learning_rate=lr)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.summary()

    # Callbacks
    model_path = os.path.join(ASSET_DIR_BASE, scenario_name, f'{scenario_name}_model.h5')
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)
    model_checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', save_best_only=True)

    # Training
    history = model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=bs,
        validation_split=0.2,
        callbacks=[early_stopping, model_checkpoint],
        verbose=1
    )

    # Muat model terbaik (dari ModelCheckpoint)
    try:
        model.load_weights(model_path)
    except:
        print(f"Warning: Gagal memuat bobot terbaik untuk {scenario_name}. Menggunakan bobot terakhir.")

    # Evaluasi
    print(f"\nEvaluasi {scenario_name} pada data test...")
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"Loss Test: {loss:.4f} | Akurasi Test: {accuracy:.4f}")

    y_pred_probs = model.predict(X_test)
    y_pred = np.argmax(y_pred_probs, axis=1)
    y_test_labels = label_encoder.inverse_transform(y_test)
    y_pred_labels = label_encoder.inverse_transform(y_pred)

    # Classification Report
    report = classification_report(y_test_labels, y_pred_labels, target_names=label_encoder.classes_, output_dict=True)
    print("\nClassification Report:")
    print(classification_report(y_test_labels, y_pred_labels, target_names=label_encoder.classes_))

    # Ekstraksi metrik rata-rata tertimbang (weighted average)
    avg_precision = report.get('weighted avg', {}).get('precision', 0.0)
    avg_recall = report.get('weighted avg', {}).get('recall', 0.0)
    avg_f1_score = report.get('weighted avg', {}).get('f1-score', 0.0)

    # Simpan Hasil dan Plot
    results = {
        'scenario': scenario_name,
        'learning_rate': lr,
        'batch_size': bs,
        'test_accuracy': accuracy,
        'test_loss': loss,
        'precision': avg_precision, # Tambahkan Precision
        'recall': avg_recall,       # Tambahkan Recall
        'f1_score': avg_f1_score,   # Tambahkan F1-Score
        'report': report,
        'history': history.history
    }

    # Simpan hasil dalam pickle
    with open(os.path.join(ASSET_DIR_BASE, scenario_name, f'{scenario_name}_results.pkl'), 'wb') as f:
        pickle.dump(results, f)

    # Plot Confusion Matrix
    cm = confusion_matrix(y_test_labels, y_pred_labels, labels=label_encoder.classes_)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix - {scenario_name}')
    plt.savefig(os.path.join(ASSET_DIR_BASE, scenario_name, f'{scenario_name}_cm.png'))
    plt.show()

    return results

# --- 6. Eksekusi Utama ---
if __name__ == '__main__':
    # Pastikan direktori aset dasar ada
    os.makedirs(ASSET_DIR_BASE, exist_ok=True)

    # 4. Persiapan Data (Hanya dilakukan sekali)
    X_train, X_test, y_train, y_test, tokenizer, label_encoder, num_classes = prepare_data()

    # Simpan aset umum (tokenizer, label_encoder)
    with open(os.path.join(ASSET_DIR_BASE, 'tokenizer.pickle'), 'wb') as f:
        pickle.dump(tokenizer, f)
    with open(os.path.join(ASSET_DIR_BASE, 'label_encoder.pickle'), 'wb') as f:
        pickle.dump(label_encoder, f)

    all_results = []

    # 7. Eksperimen 6 Skenario
    for params in SCENARIOS:
        result = train_and_evaluate_model(X_train, X_test, y_train, y_test, tokenizer, label_encoder, num_classes, params)
        all_results.append(result)

    print("\n\n--- Ringkasan Semua Skenario ---")
    summary_df = pd.DataFrame([
        {
            'Skenario': r['scenario'],
            'BS': r['batch_size'],
            'LR': r['learning_rate'],
            'Akurasi Test': f"{r['test_accuracy']:.4f}",
            'Precision': f"{r['precision']:.4f}", # Tambahkan Precision
            'Recall': f"{r['recall']:.4f}",       # Tambahkan Recall
            'F1-Score': f"{r['f1_score']:.4f}"    # Tambahkan F1-Score
        }
        for r in all_results
    ])
    print(summary_df)

    # Simpan ringkasan ke file CSV
    summary_df.to_csv(os.path.join(ASSET_DIR_BASE, 'summary_all_scenarios.csv'), index=False)
    print(f"\nRingkasan lengkap juga disimpan di: {os.path.join(ASSET_DIR_BASE, 'summary_all_scenarios.csv')}")