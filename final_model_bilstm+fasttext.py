# -*- coding: utf-8 -*-
"""Salinan dari model_bilstm+fasttext.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10m0iQgVcdmka2aGaSUaX-HktBlH0YkTb
"""

# Pastikan library yang dibutuhkan terinstal
!pip install -q fasttext
!pip install -q Sastrawi
!pip install -q nlpaug

# --- 1. Import dan Setup ---
import os
import re
import pickle
import numpy as np
import pandas as pd
import fasttext
import random

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import nlpaug.augmenter.word as naw # Diperlukan untuk augmentasi FastText

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix

import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# --- 2. Konfigurasi Global & Hyperparameter ---
DATA_PATH = '/content/drive/MyDrive/skripsi/cleaned_tweets2.csv'
AUGMENTED_DATA_PATH = '/content/drive/MyDrive/skripsi/augmented_tweets.csv' # File hasil augmentasi
FASTTEXT_BIN_PATH = '/content/drive/MyDrive/skripsi/cc.id.300.bin'
FASTTEXT_VEC_PATH = '/content/drive/MyDrive/skripsi/cc.id.300.vec'
ASSET_DIR_BASE = '/content/drive/MyDrive/skripsi/fasttext_bilstm_scenarios'
TEST_DATA_PATH = os.path.join(ASSET_DIR_BASE, 'global_test_data_ft.csv')

# Hyperparameter yang konsisten
EMBEDDING_DIM = 300
MAX_SEQUENCE_LENGTH = 100
EPOCHS = 20
TEST_SIZE = 0.2
DROPOUT_RATE = 0.3
LSTM_UNITS = 128
NUM_AUGMENTATIONS_PER_SAMPLE = 1 # Jumlah augmentasi per sampel
AUG_P = 0.05 # Probabilitas augmentasi per kata

# KONTROL AUGMENTASI: Set ke True untuk melakukan augmentasi ulang
USE_AUGMENTATION = False # Dipertahankan dari kode Anda

# Hyperparameter yang divariasikan (6 Skenario)
SCENARIOS = [
    {'batch_size': 64, 'learning_rate': 0.001, 'name': 'Skenario_1_BS64_LR001'},
    {'batch_size': 64, 'learning_rate': 0.01, 'name': 'Skenario_2_BS64_LR01'},
    {'batch_size': 64, 'learning_rate': 0.0001, 'name': 'Skenario_3_BS64_LR0001'},
    {'batch_size': 128, 'learning_rate': 0.001, 'name': 'Skenario_4_BS128_LR001'},
    {'batch_size': 128, 'learning_rate': 0.01, 'name': 'Skenario_5_BS128_LR01'},
    {'batch_size': 128, 'learning_rate': 0.0001, 'name': 'Skenario_6_BS128_LR0001'},
]

if not os.path.exists(ASSET_DIR_BASE):
    os.makedirs(ASSET_DIR_BASE)

# --- 3. Preprocessing Functions ---
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Muat slang dictionary
slang_dict_path = '/content/drive/MyDrive/skripsi/slang.csv'
def load_slang_dict(path=None):
    default = {
        'yg': 'yang', 'kmrn': 'kemarin', 'udh': 'sudah', 'tdk': 'tidak', 'jg': 'juga',
        'ga': 'tidak', 'gk': 'tidak', 'bgt': 'banget', 'bgtu': 'begitu', 'trs': 'terus',
        'jd': 'jadi', 'dr': 'dari', 'klo': 'kalau', 'sm': 'sama', 'gmn': 'bagaimana',
        'utk': 'untuk', 'krn': 'karena', 'brp': 'berapa', 'sy': 'saya', 'dn': 'dan',
        'skrg': 'sekarang', 'bikin': 'membuat', 'bln': 'bulan', 'pke': 'pakai',
        'org': 'orang', 'dgn': 'dengan', 'pdhl': 'padahal', 'ato': 'atau',
        'sbnrnya': 'sebenarnya', 'tp': 'tapi', 'jdwal': 'jadwal', 'jgk': 'juga',
        'mnrt': 'menurut', 'blm': 'belum', 'bbrp': 'beberapa',
        'gt': 'begitu', 'dulu': 'dahulu', 'k': 'ke', 'lg': 'lagi', 'tggl': 'tinggal', 'bkn': 'bukan'
    }
    if path and os.path.exists(path):
        try:
            df_s = pd.read_csv(path)
            if 'slang' in df_s.columns and 'formal' in df_s.columns:
                return dict(zip(df_s['slang'], df_s['formal']))
        except Exception:
            pass
    return default
slang_dict = load_slang_dict(slang_dict_path)

def clean_and_normalize_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    words = text.split()
    normalized_words = [slang_dict.get(w, w) for w in words]
    text = ' '.join(normalized_words)
    text = stemmer.stem(text)
    return text

# --- 4. Persiapan Data (Inti Sederhana dengan Kontrol Augmentasi) ---
def prepare_data():
    print("Mempersiapkan data dan tokenizing...")
    df = None

    # --- LANGKAH 1: Kontrol Pemuatan Data Augmented ---
    if os.path.exists(AUGMENTED_DATA_PATH) and not USE_AUGMENTATION:
        try:
            df = pd.read_csv(AUGMENTED_DATA_PATH)
            # Fokus hanya pada 'cleaned_text' dan 'label'
            df = df[['text', 'label']].copy()
            print(f"Data augmented dimuat dari {AUGMENTED_DATA_PATH}. Jumlah data: {len(df)}")
            return process_encoding_and_split(df)
        except Exception:
            df = None # Lanjut ke pemuatan data asli jika gagal

    # --- LANGKAH 2: Muat Data Asli dan Lakukan Augmentasi jika diperlukan ---
    if df is None or USE_AUGMENTATION:
        if not os.path.exists(DATA_PATH):
            raise FileNotFoundError(f"File data asli tidak ditemukan di {DATA_PATH}")

        raw = pd.read_csv(DATA_PATH)
        # Langsung ambil kolom yang diasumsikan bersih
        df_base = raw[['text', 'label']].copy()

        # Lakukan Augmentasi FastText JIKA USE_AUGMENTATION=True atau file augmented tidak ada
        if USE_AUGMENTATION or not os.path.exists(AUGMENTED_DATA_PATH):
            if USE_AUGMENTATION: # Hanya cetak ini jika diaktifkan secara eksplisit
                print(f"Melakukan augmentasi FastText berbasis nlpaug (x{NUM_AUGMENTATIONS_PER_SAMPLE})...")

            aug = None
            try:
                # Inisialisasi augmenter FastText
                if os.path.exists(FASTTEXT_BIN_PATH):
                    aug = naw.WordEmbsAug(model_type='fasttext', model_path=FASTTEXT_BIN_PATH, action="substitute", aug_p=AUG_P)
                elif os.path.exists(FASTTEXT_VEC_PATH):
                    aug = naw.WordEmbsAug(model_type='fasttext', model_path=FASTTEXT_VEC_PATH, action="substitute", aug_p=AUG_P)
                else:
                    print("Model fasttext untuk augmenter tidak ditemukan; augmentasi dinonaktifkan.")
            except Exception:
                aug = None

            if aug and NUM_AUGMENTATIONS_PER_SAMPLE > 0:
                augmented_data = []
                for _, row in df_base.iterrows():
                    original = row['text']
                    lbl = row['label']
                    augmented_data.append({'text': original, 'label': lbl})
                    for _ in range(NUM_AUGMENTATIONS_PER_SAMPLE):
                        try:
                            aug_texts = aug.augment(original)
                            if aug_texts:
                                augmented_data.append({'text': aug_texts[0], 'label': lbl})
                        except Exception:
                            pass

                df = pd.DataFrame(augmented_data)

            else:
                 df = df_base # Gunakan data dasar jika augmentasi tidak berjalan

            # Simpan hasil (baik data dasar maupun yang di-augmentasi)
            df.to_csv(AUGMENTED_DATA_PATH, index=False, encoding='utf-8')
            print(f"Data diproses dan disimpan ke {AUGMENTED_DATA_PATH}. Jumlah data: {len(df)}")

    # --- LANGKAH 3: Panggil Proses Encoding dan Split ---
    return process_encoding_and_split(df)

def process_encoding_and_split(df):
    # Fungsi ini memisahkan proses encoding dari pemuatan data
    label_encoder = LabelEncoder()
    df['encoded_label'] = label_encoder.fit_transform(df['label'])
    num_classes = len(label_encoder.classes_)

    tokenizer = Tokenizer(oov_token="<unk>")
    tokenizer.fit_on_texts(df['text'].astype(str))
    word_index = tokenizer.word_index
    sequences = tokenizer.texts_to_sequences(df['text'].astype(str))
    padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')

    # Split Data
    X_train, X_test, y_train, y_test = train_test_split(
        padded_sequences, df['encoded_label'], test_size=TEST_SIZE,
        random_state=42, stratify=df['encoded_label']
    )

    test_df = df.iloc[y_test.index][['text', 'label']].reset_index(drop=True)
    test_df.to_csv(TEST_DATA_PATH, index=False)

    print(f"\nTotal data train: {len(X_train)} | Total data test: {len(X_test)}")
    return X_train, X_test, y_train, y_test, tokenizer, label_encoder, num_classes

# --- 5. Muat FastText dan Embedding Matrix (Hanya Sekali) ---
def load_fasttext_embedding(tokenizer):
    print("\nMemuat model FastText dan membuat embedding matrix...")
    ft_model = None
    try:
        if os.path.exists(FASTTEXT_BIN_PATH):
            ft_model = fasttext.load_model(FASTTEXT_BIN_PATH)
        elif os.path.exists(FASTTEXT_VEC_PATH):
            ft_model = fasttext.load_model(FASTTEXT_VEC_PATH)
    except Exception:
        ft_model = None

    word_index = tokenizer.word_index
    vocab_size = len(word_index) + 1
    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))

    if ft_model is not None:
        for word, i in word_index.items():
            try:
                embedding_matrix[i] = ft_model.get_word_vector(word)
            except Exception:
                embedding_matrix[i] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM,))
    else:
        print("FastText tidak tersedia: inisialisasi embedding acak.")
        embedding_matrix = np.random.normal(size=(vocab_size, EMBEDDING_DIM))

    print("Embedding matrix shape:", embedding_matrix.shape)
    return embedding_matrix

# --- 6. Definisi Model dan Fungsi Pelatihan ---
def build_model(embedding_matrix, max_seq_len, lstm_units, dropout_rate, num_classes):
    vocab_size = embedding_matrix.shape[0]
    embedding_dim = embedding_matrix.shape[1]

    input_layer = Input(shape=(max_seq_len,), dtype='int32')
    embedding_layer = Embedding(input_dim=vocab_size,
                                output_dim=embedding_dim,
                                weights=[embedding_matrix],
                                input_length=max_seq_len,
                                trainable=True)(input_layer)

    bilstm_layer = Bidirectional(LSTM(lstm_units, return_sequences=False))(embedding_layer)
    dropout_layer = Dropout(dropout_rate)(bilstm_layer)
    output_layer = Dense(num_classes, activation='softmax')(dropout_layer)

    model = Model(inputs=input_layer, outputs=output_layer)
    return model

def train_and_evaluate_model(X_train, X_test, y_train, y_test, embedding_matrix, label_encoder, num_classes, params):
    scenario_name = params['name']
    lr = params['learning_rate']
    bs = params['batch_size']

    print(f"\n--- Memulai {scenario_name} (BS: {bs}, LR: {lr}) ---")
    os.makedirs(os.path.join(ASSET_DIR_BASE, scenario_name), exist_ok=True)

    model = build_model(
        embedding_matrix=embedding_matrix,
        max_seq_len=MAX_SEQUENCE_LENGTH,
        lstm_units=LSTM_UNITS,
        dropout_rate=DROPOUT_RATE,
        num_classes=num_classes
    )

    optimizer = Adam(learning_rate=lr)
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.summary()

    model_path = os.path.join(ASSET_DIR_BASE, scenario_name, f'{scenario_name}_model.h5')
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)
    model_checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)

    history = model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=bs,
        validation_split=0.2,
        callbacks=[early_stopping, model_checkpoint],
        verbose=1
    )

    try:
        model.load_weights(model_path)
    except:
        print(f"Warning: Gagal memuat bobot terbaik untuk {scenario_name}. Menggunakan bobot terakhir.")

    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    y_pred_probs = model.predict(X_test)
    y_pred = np.argmax(y_pred_probs, axis=1)
    y_test_labels = label_encoder.inverse_transform(y_test)
    y_pred_labels = label_encoder.inverse_transform(y_pred)

    report = classification_report(y_test_labels, y_pred_labels, target_names=label_encoder.classes_, output_dict=True)

    results = {
        'scenario': scenario_name,
        'learning_rate': lr,
        'batch_size': bs,
        'test_accuracy': accuracy,
        'test_loss': loss,
        'report': report,
        'history': history.history
    }

    with open(os.path.join(ASSET_DIR_BASE, scenario_name, f'{scenario_name}_results.pkl'), 'wb') as f:
        pickle.dump(results, f)

    cm = confusion_matrix(y_test_labels, y_pred_labels, labels=label_encoder.classes_)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix - {scenario_name}')
    plt.savefig(os.path.join(ASSET_DIR_BASE, scenario_name, f'{scenario_name}_cm.png'))
    plt.show()

    print(f"Loss Test: {loss:.4f} | Akurasi Test: {accuracy:.4f}")
    print(classification_report(y_test_labels, y_pred_labels, target_names=label_encoder.classes_))

    return results

# --- 7. Eksekusi Utama ---
if __name__ == '__main__':

    X_train, X_test, y_train, y_test, tokenizer, label_encoder, num_classes = prepare_data()

    embedding_matrix = load_fasttext_embedding(tokenizer)

    # Simpan aset umum (tokenizer, label_encoder, slang_dict)
    with open(os.path.join(ASSET_DIR_BASE, 'tokenizer.pickle'), 'wb') as f:
        pickle.dump(tokenizer, f)
    with open(os.path.join(ASSET_DIR_BASE, 'label_encoder.pickle'), 'wb') as f:
        pickle.dump(label_encoder, f)
    with open(os.path.join(ASSET_DIR_BASE, 'slang_dict.pickle'), 'wb') as f:
        pickle.dump(slang_dict, f)

    all_results = []

    for params in SCENARIOS:
        result = train_and_evaluate_model(X_train, X_test, y_train, y_test, embedding_matrix, label_encoder, num_classes, params)
        all_results.append(result)

    print("\n\n--- Ringkasan Semua Skenario ---")
    summary_df = pd.DataFrame([
        {'Skenario': r['scenario'], 'BS': r['batch_size'], 'LR': r['learning_rate'], 'Akurasi Test': f"{r['test_accuracy']:.4f}"}
        for r in all_results
    ])
    print(summary_df)

    print("\nâœ… Semua skenario telah dieksekusi dan hasilnya disimpan.")